{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movie Reviews using LSTM RNN Model\n",
    "\n",
    "- 0 - negative\n",
    "\n",
    "- 1 - somewhat negative\n",
    "\n",
    "- 2 - neutral\n",
    "\n",
    "- 3 - somewhat positive\n",
    "\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t', header=0)\n",
    "test = pd.read_csv('test.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156060, 4), (66292, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_docs_train = train['Phrase'].values\n",
    "raw_docs_test = test['Phrase'].values\n",
    "sentiment_train = train['Sentiment'].values\n",
    "num_labels = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', ',', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', '[', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', '(', '{', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', '\"', u'from', u'her', u'their', u'aren', u'there', u'been', '.', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', ';', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', ':', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', ')', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', ']', u'why', u'a', u'off', \"'\", u'i', u'm', u'yours', u'so', u'y', u'the', '}', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n",
      "\n",
      "\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n",
      "\n",
      "\n",
      "['A', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', ',', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story', '.']\n",
      "\n",
      "\n",
      "['A', 'series', 'escapades', 'demonstrating', 'adage', 'good', 'goose', 'also', 'good', 'gander', 'occasionally', 'amuses', 'none', 'amounts', 'much', 'story']\n",
      "\n",
      "\n",
      "['a', u'seri', u'escapad', u'demonstr', u'adag', u'good', u'goos', u'also', u'good', u'gander', u'occasion', u'amus', u'none', u'amount', u'much', u'stori']\n"
     ]
    }
   ],
   "source": [
    "print \"pre-processing train docs...\"\n",
    "processed_docs_train = []\n",
    "for index, doc in enumerate(raw_docs_train):\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_train.append(stemmed)\n",
    "    \n",
    "    if index == 0:\n",
    "        print ('\\n')\n",
    "        print (doc)\n",
    "        print ('\\n')\n",
    "        print (tokens)\n",
    "        print ('\\n')\n",
    "        print (filtered)\n",
    "        print ('\\n')\n",
    "        print (stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing test docs...\n"
     ]
    }
   ],
   "source": [
    "print \"pre-processing test docs...\"\n",
    "processed_docs_test = []\n",
    "for doc in raw_docs_test:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_test.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_docs_all = np.concatenate((processed_docs_train, processed_docs_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  13759\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_docs_all)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "print \"dictionary size: \", dictionary_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'a', u'seri')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0], dictionary[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to token ids...\n",
      "['a', u'seri', u'escapad', u'demonstr', u'adag', u'good', u'goos', u'also', u'good', u'gander', u'occasion', u'amus', u'none', u'amount', u'much', u'stori']\n",
      "[0, 14, 12, 13, 11, 2, 9, 6, 2, 5, 10, 4, 1, 3, 8, 7]\n",
      "[[0, 14, 12, 13, 11, 2, 9, 6, 2, 5, 10, 4, 1, 3, 8, 7]]\n",
      "[16]\n"
     ]
    }
   ],
   "source": [
    "print \"converting to token ids...\"\n",
    "word_id_train, word_id_len = [], []\n",
    "for index,doc in enumerate(processed_docs_train):\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_train.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "    \n",
    "    if index == 0:\n",
    "        print (doc)\n",
    "        print (word_ids)\n",
    "        print (word_id_train)\n",
    "        print (word_id_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_id_test, word_ids = [], []\n",
    "for doc in processed_docs_test:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_test.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.16991976686\n",
      "3.80478385787\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "seq_len = np.round((np.mean(word_id_len) + 2*np.std(word_id_len))).astype(int)\n",
    "print (np.mean(word_id_len))\n",
    "print (np.std(word_id_len))\n",
    "print (seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "word_id_train = sequence.pad_sequences(np.array(word_id_train), maxlen=seq_len)\n",
    "word_id_test = sequence.pad_sequences(np.array(word_id_test), maxlen=seq_len)\n",
    "y_train_enc = np_utils.to_categorical(sentiment_train, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   11     2     9 ...,     3     8     7]\n",
      " [    0     0     0 ...,    11     2     9]\n",
      " [    0     0     0 ...,     0     0    14]\n",
      " ..., \n",
      " [    0     0     0 ...,     0 11849 11850]\n",
      " [    0     0     0 ...,     0     0 11849]\n",
      " [    0     0     0 ...,     0     0 11850]]\n"
     ]
    }
   ],
   "source": [
    "print (word_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print (y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM RNN Model\n",
    "\n",
    "[Long short-term memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) is a recurrent neural network (RNN) architecture that remembers values over arbitrary intervals. Stored values are not modified as learning proceeds. RNNs allow forward and backward connections between neurons.\n",
    "\n",
    "An LSTM network contains LSTM units instead of, or in addition to, other network units. An LSTM unit remembers values for either long or short time periods. The key to this ability is that it uses no activation function within its recurrent components. Thus, the stored value is not iteratively modified and the gradient does not tend to vanish when trained with backpropagation through time.\n",
    "\n",
    "`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "> **units:** Positive integer, dimensionality of the output space.\n",
    ">\n",
    "> **dropout:** Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
    ">\n",
    "> **recurrent_dropout:** Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    ">\n",
    "> *Source: https://keras.io/layers/recurrent/#lstm*\n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "[Embedding Layer](https://medium.com/towards-data-science/deep-learning-4-embedding-layers-f9a02d55ac12) is used to:\n",
    "\n",
    "- One-hot encoded vectors are high-dimensional and sparse. Let's assume that we are doing Natural Language Processing (NLP) and have a dictionary of 2000 words. This means that, when using one-hot encoding, each word will be represented by a vector containing 2000 integers. And 1999 of these integers are zeros. In a big dataset this approach is not computationally efficient.\n",
    "\n",
    "\n",
    "- The vectors of each embedding get updated while training the neural network. This allows us to visualize relationships between words, but also between everything that can be turned into a vector through an embedding layer.\n",
    "\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    ">Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    ">\n",
    ">This layer can only be used as the first layer in a model.\n",
    ">\n",
    "> *Source: https://keras.io/layers/embeddings/*\n",
    "\n",
    "*Example:*\n",
    "\n",
    "`model.add(Embedding(1000, 64, input_length=10))`\n",
    "\n",
    "In the above example code, the model will take as input an integer matrix of size (batch, input_length). The largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size). \n",
    "Now `model.output_shape == (None, 10, 64)`, where None is the batch dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LSTM ...\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "print \"fitting LSTM ...\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionary_size, 128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "156060/156060 [==============================] - 422s - loss: 0.9993 - acc: 0.5981   \n",
      "Epoch 2/3\n",
      "156060/156060 [==============================] - 450s - loss: 0.8258 - acc: 0.6625   \n",
      "Epoch 3/3\n",
      "156060/156060 [==============================] - 380s - loss: 0.7732 - acc: 0.6798   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe5d259b50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_id_train, y_train_enc, epochs=3, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolutional Neural Network (CNN) + LSTM\n",
    "\n",
    "We add a one-dimensional CNN *Conv1D()* and a max pooling layer *MaxPooling1D()* after the Embedding layer which then feed the features to the LSTM. We use a set of 32 features with filter length of 3. The pooling layer has the standard length of 2 to halve the feature map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LSTM ...\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "print \"fitting LSTM ...\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionary_size, 128))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# sigmoid activation for binary classification\n",
    "# softmax activation for multi-class classification\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "156060/156060 [==============================] - 228s - loss: 1.0164 - acc: 0.5926   \n",
      "Epoch 2/3\n",
      "156060/156060 [==============================] - 242s - loss: 0.8160 - acc: 0.6635   \n",
      "Epoch 3/3\n",
      "156060/156060 [==============================] - 219s - loss: 0.7515 - acc: 0.6867   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe9d4f5990>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_id_train, y_train_enc, epochs=3, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 61s    \n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(word_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a submission\n",
    "test['Sentiment'] = test_pred.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test.to_csv('./submission_lstm_cnn.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://github.com/vsmolyakov/kaggle/blob/master/sentiment/sentiment_kernel.py\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "[Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
